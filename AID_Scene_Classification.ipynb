{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN/to1v9aoYUMvbYb28DIIG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lewisdoukas/scene-classification/blob/main/AID_Scene_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip '/content/gdrive/MyDrive/aidProject/data/NWPU-RESISC45_test.zip' -d '/content/gdrive/MyDrive/aidProject/data'"
      ],
      "metadata": {
        "id": "8GeVQ53AZuWz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xNJ1bYrgwOCd"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/gdrive\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, models, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import timm\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import os"
      ],
      "metadata": {
        "id": "05BZtNBUHQwg"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ROOT_DIR = \"/content/gdrive/MyDrive/aidProject\"\n",
        "DATA_DIR = os.path.join(ROOT_DIR, \"data\")\n",
        "OUTPUT_DIR = os.path.join(ROOT_DIR, \"output\")\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "CLASS_NAMES = [\"Airport\", \"Bridge\", \"Center\", \"Industrial\", \"Port\", \"RailwayStation\", \"StorageTanks\", \"Viaduct\"]\n",
        "NUM_CLASSES = len(CLASS_NAMES) # 8\n",
        "NUM_EPOCHS = 20\n",
        "LEARNING_RATE = 0.001\n",
        "# Lower LR for transformers\n",
        "# LEARNING_RATE = 3e-5\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "khTtcC1JHtU_"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Preprocessing (ImageNet default values)\n",
        "transform = {\n",
        "    \"train\": transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomRotation(10),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    \"val\": transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "}"
      ],
      "metadata": {
        "id": "HuBH-O7nIBJr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load datasets\n",
        "train_dataset = datasets.ImageFolder(os.path.join(DATA_DIR, \"train\"), transform=transform[\"train\"])\n",
        "val_dataset = datasets.ImageFolder(os.path.join(DATA_DIR, \"val\"), transform=transform[\"val\"])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)"
      ],
      "metadata": {
        "id": "R76w7seFIOtI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Pretrained VGG-16\n",
        "model = models.vgg16(pretrained=True)\n",
        "num_features = model.classifier[6].in_features\n",
        "model.classifier[6] = nn.Linear(num_features, NUM_CLASSES)\n",
        "model = model.to(DEVICE)"
      ],
      "metadata": {
        "id": "rKeYW4zxUTxt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Pretrained ResNet-50\n",
        "model = models.resnet50(pretrained=True)\n",
        "num_features = model.fc.in_features\n",
        "model.fc = nn.Linear(num_features, NUM_CLASSES)\n",
        "model = model.to(DEVICE)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "W-omkOVUIY0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Pretrained ViT Model\n",
        "model = timm.create_model(\"vit_base_patch16_224\", pretrained=True, num_classes=NUM_CLASSES)\n",
        "model = model.to(DEVICE)"
      ],
      "metadata": {
        "id": "ugITdem8eLo1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hybrid CNN-Transformer Model\n",
        "class HybridCNNTransformer(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(HybridCNNTransformer, self).__init__()\n",
        "\n",
        "        # CNN Backbone (ResNet-50)\n",
        "        self.cnn = models.resnet50(pretrained=True)\n",
        "        self.cnn.fc = nn.Identity()  # Remove final FC layer\n",
        "\n",
        "        # Transformer Backbone (ViT)\n",
        "        self.transformer = timm.create_model(\"vit_base_patch16_224\", pretrained=True)\n",
        "        self.transformer.head = nn.Identity()  # Remove final classifier\n",
        "\n",
        "        # Fully Connected Layer for Classification\n",
        "        self.fc = nn.Linear(2048 + 768, num_classes)  # ResNet-50 (2048) + ViT (768)\n",
        "\n",
        "    def forward(self, x):\n",
        "        cnn_features = self.cnn(x)  # Extract CNN features\n",
        "        transformer_features = self.transformer(x)  # Extract Transformer features\n",
        "        combined = torch.cat((cnn_features, transformer_features), dim=1)  # Concatenate features\n",
        "        output = self.fc(combined)  # Final classification\n",
        "        return output\n",
        "\n",
        "\n",
        "model = HybridCNNTransformer(num_classes=NUM_CLASSES)\n",
        "model = model.to(DEVICE)"
      ],
      "metadata": {
        "id": "MTMebeqhUDZ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss and Optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "# optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4) # ViT"
      ],
      "metadata": {
        "id": "dtxTVXkMIeEz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train & Validate model\n",
        "def apply_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, class_names, model_filename):\n",
        "    best_acc = 0.0\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        train_loss, correct_train = 0.0, 0\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct_train += torch.sum(preds == labels.data)\n",
        "\n",
        "        train_acc = correct_train.double() / len(train_loader.dataset)\n",
        "        print(f\"Train Loss: {train_loss/len(train_loader):.4f}, Train Acc: {train_acc:.4f}\")\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "\n",
        "        correct_val = 0\n",
        "        y_true, y_pred = [], []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for images, labels in val_loader:\n",
        "                images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "                outputs = model(images)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                correct_val += torch.sum(preds == labels.data)\n",
        "                y_true.extend(labels.cpu().numpy())\n",
        "                y_pred.extend(preds.cpu().numpy())\n",
        "\n",
        "        # Metrics\n",
        "        val_acc = correct_val.double() / len(val_loader.dataset)\n",
        "        precision = precision_score(y_true, y_pred, average='macro')\n",
        "        recall = recall_score(y_true, y_pred, average='macro')\n",
        "        f1 = f1_score(y_true, y_pred, average='macro')\n",
        "        print(f\"\\nValidation Acc: {val_acc:.4f}\")\n",
        "        print(f\"Precision: {precision:.4f}\")\n",
        "        print(f\"Recall: {recall:.4f}\")\n",
        "        print(f\"F1-score: {f1:.4f}\")\n",
        "\n",
        "        # Save best model\n",
        "        if val_acc > best_acc:\n",
        "            best_acc = val_acc\n",
        "            torch.save(model.state_dict(), model_filename)\n",
        "\n",
        "            # Calculate confusion matrix for best model\n",
        "            cm = confusion_matrix(y_true, y_pred)\n",
        "            cm_filename = model_filename.replace(\".pth\", \"_cm.png\")\n",
        "\n",
        "            plt.figure(figsize=(10, 8))\n",
        "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
        "            plt.xlabel(\"Predicted\")\n",
        "            plt.ylabel(\"Actual\")\n",
        "            plt.title(\"Confusion Matrix\")\n",
        "            plt.savefig(cm_filename)\n",
        "\n",
        "            print(\"Model saved...\")"
      ],
      "metadata": {
        "id": "7SK3h_QkIhQY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model_filename = os.path.join(OUTPUT_DIR, \"best_aid_vgg16_10.pth\")\n",
        "model_filename = os.path.join(OUTPUT_DIR, \"best_aid_resnet50_20.pth\")\n",
        "# model_filename = os.path.join(OUTPUT_DIR, \"best_aid_vit_10.pth\")\n",
        "# model_filename = os.path.join(OUTPUT_DIR, \"best_aid_hybrid_10.pth\")\n",
        "\n",
        "apply_model(model, train_loader, val_loader, criterion, optimizer, NUM_EPOCHS, CLASS_NAMES, model_filename)"
      ],
      "metadata": {
        "id": "7ku0AcwXImeW",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test best model using NWPU-RESISC45 overlapping classes\n",
        "MODEL_PATH = os.path.join(OUTPUT_DIR, \"best_aid_hybrid_10.pth\")\n",
        "TEST_DATA_DIR = os.path.join(DATA_DIR, \"NWPU-RESISC45_test\")\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "gNo2qJqW_Jqt"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Trained Model\n",
        "model = HybridCNNTransformer(num_classes=NUM_CLASSES)\n",
        "model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
        "model = model.to(DEVICE)\n",
        "model.eval()  # Set model to evaluation mode\n",
        "print(\"Model Loaded Successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "beOsNfrzAwov",
        "outputId": "2a7e5017-fad3-4664-a4fc-4e802f398dc8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Loaded Successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])"
      ],
      "metadata": {
        "id": "jU1LzCBzA_H-"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = datasets.ImageFolder(TEST_DATA_DIR, transform=transform)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "print(f\"Loaded {len(test_dataset)} images from common classes.\")"
      ],
      "metadata": {
        "id": "NfkGdyD6BGKy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate Model\n",
        "correct_val = 0\n",
        "y_true, y_pred = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "        outputs = model(images)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        correct_val += torch.sum(preds == labels.data)\n",
        "        y_true.extend(labels.cpu().numpy())\n",
        "        y_pred.extend(preds.cpu().numpy())\n",
        "\n",
        "# Metrics\n",
        "val_acc = correct_val.double() / len(test_loader.dataset)\n",
        "precision = precision_score(y_true, y_pred, average='macro')\n",
        "recall = recall_score(y_true, y_pred, average='macro')\n",
        "f1 = f1_score(y_true, y_pred, average='macro')\n",
        "print(f\"\\nModel Accuracy on NWPU-RESISC45 (Common Classes):\")\n",
        "print(f\"Validation Acc: {val_acc:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-score: {f1:.4f}\")\n",
        "\n",
        "\n",
        "# Calculate confusion matrix\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "cm_filename = os.path.join(OUTPUT_DIR, \"test_hybrid_NWPU_RESISC45.png\")\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Oranges', xticklabels=CLASS_NAMES, yticklabels=CLASS_NAMES)\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.savefig(cm_filename)"
      ],
      "metadata": {
        "id": "s73KeK4JBp84"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}